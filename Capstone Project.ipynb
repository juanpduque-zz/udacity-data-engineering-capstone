{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Capstone Project\n",
    "\n",
    "#### Summary\n",
    "This project try to demonstrate the use of the skills associated with data engineering projects with an exercise that perform an analysis of US immigration, seeing by the type of visas and the profiles associated. For the exercise the data sources are:  visatype, gender, port_of_entry, nationality and month aggregated across numerous dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? \n",
    "\n",
    "#### Description\n",
    "\n",
    "1. **I94 Immigration Data:** Contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). [travel.trade.gov](https://travel.trade.gov/research/reports/i94/historical/2016.html)<br>\n",
    "2. **World Temperature Data:** Includes data on temperature changes in the U.S. since 1850..[kaggle.com](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)<br>\n",
    "3. **U.S. City Demographic Data:** Contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. [opendatasoft.com](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/)<br>\n",
    "4. **Airport Code Table:** A simple table of airport codes and corresponding cities. [datahub.io](https://datahub.io/core/airport-codes#data)\"\n",
    "   ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Storage \n",
    "\n",
    "Data was stored in S3 buckets in a collection of CSV and PARQUET files. The immigration dataset extends to several million rows and thus this dataset was converted to PARQUET files to allow for easy data manipulation through to Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Model\n",
    "\n",
    "Data was stored in S3 buckets in a collection of CSV and PARQUET files. The immigration dataset extends to several million rows and thus this dataset was converted to PARQUET files to allow for easy data manipulation through to Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Model](/Images/data_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETL Pipeline\n",
    "\n",
    "Defining the data model and creating the star schema involves various steps, made significantly easier through the use of Airflow. The process of extracting, transforming the data and then writing CSV and PARQUET files to Redshift is accomplished through various tasks highlighted below in the ETL Dag graph.\n",
    "\n",
    "These steps include: \n",
    "- Extracting data from SAS Documents and writing as CSV files\n",
    "- Extracting remaining CSV and PARQUET files\n",
    "- Create the table in Redshift\n",
    "- Writing CSV and PARQUET files to Redshift\n",
    "- Performing data quality checks on the newly created tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Pipeline](Images/data_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Checks\n",
    "\n",
    "The following ensures that the data has been inserted correctly.\n",
    "\n",
    "We will use a data quality check for every table to ensure data was inserted.\n",
    "\n",
    "With the use of 'DataQualityOperator' Operator, which ensures that for a given 'table_name', there exists records.\n",
    "\n",
    "Then with the use of 'EnsureDistinctRecords' we will ensure that the immigrations, airport_codes, cities and countries tables have data uniqueness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenarios\n",
    "\n",
    "- Data increase by 100x. read > write. write > read\n",
    "\n",
    "  - **Redshift:** Analytical database, optimized for aggregation, also good performance for read-heavy workloads\n",
    "\n",
    "- Pipelines would be run on 7am daily. how to update dashboard? would it still work?\n",
    "\n",
    "  - DAG retries, or send emails on failures\n",
    "  - Daily intervals with quality checks\n",
    "  - Look at DAG logs to figure out what went wrong\n",
    "\n",
    "- Make it available to 100+ people\n",
    "  - Redshift with auto-scaling capabilities and good read performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
